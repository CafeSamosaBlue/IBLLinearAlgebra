
From here on out, we will only be considering linear transformations with the same domain
and codomain (i.e., transformations $\mathcal T:\R^n\to\R^n$). Why? Because that will allow us
to \emph{compare} input and output vectors. From this perspective, a linear transformation $\mathcal T:\R^n\to\R^n$
may stretch, twist, shear, rotate, and project vectors. 

XXX Figure with stretched and sheared vectors

It's the stretched vectors that we're most interested in now. If $\mathcal T$ stretches the vector $\vec v$,
then $\mathcal T$, in that direction, can be described by $\vec v\mapsto \alpha\vec v$, which is an easy-to-understand
linear transformation. The ``stretch'' directions for a linear transformation have a special name---\emph{eigen directions}---and
the vectors that are stretched are called \emph{eigenvectors}.

\SavedDefinitionRender{Eigenvector}

The word \emph{eigen} is German for characteristic, representative, or intrinsic, and
we will see that eigenvectors provide one of the best contexts in which to understand a linear transformation.

\begin{example}
	Let $\mathcal P:\R^2\to\R^2$ be projection onto the line $\ell$ given by $y=x$.
	Find the eigenvectors and eigenvalues of $\mathcal P$.

	We are looking for vectors $\vec v\neq \vec 0$ such that $\mathcal P\vec v=\lambda \vec v$ for some $\lambda$.
	Since $\mathcal P(\ell)=\ell$, we know for any $\vec v\in \ell$
	\[
		\mathcal P(\vec v)=1\vec v=\vec v.
	\]
	Therefore, any non-zero multiple of $\mat{1\\1}$ is an eigenvector for $\mathcal P$ with corresponding
	eigenvalue $1$.

	By considering the null space of $\mathcal P$, we see, for example,
	\[
		\mathcal P\mat{1\\-1}=\mat{0\\0}=0\mat{1\\-1},
	\]
	and so $\mat{1\\-1}$ and all its non-zero multiples are eigenvectors of $\mathcal P$ with corresponding
	eigenvalue $0$.
\end{example}

\Heading{Finding Eigenvectors}

Sometimes you can find the eigenvectors/values of a linear transformation just by thinking about it.
For example, for reflections, projections, and dilations eigenvectors, the eigen directions
are geometrically clear. However, for an arbitrary matrix transformation, it may not be obvious.

Our goal now will be to see if we can leverage our other linear algebra knowledge to find eigenvectors/values,
particularly for matrix transformations. So that we don't have to switch back and forth
between thinking about linear transformations and thinking about matrices, let just think about matrices for now.

Let $M$ be a square matrix. The vector $\vec v\neq \vec 0$
is an eigenvector for $M$ if and only if there exists a scalar $\lambda$ so that
\begin{equation}
	\label{EQEIGEN}
	M\vec v=\lambda \vec v.
\end{equation}
Put another way, $\vec v\neq \vec 0$ is an eigenvector for $M$ if and only if
\[
	M\vec v-\lambda \vec v=(M-\lambda I)\vec v=\vec 0.
\]
The middle equation provides a key insight. The operation $\vec v\mapsto M\vec v-\lambda\vec v$ can be achieved
by multiplying $\vec v$ by the single matrix $E_\lambda=M-\lambda I$.

Now we have that $\vec v\neq \vec 0$ is an eigenvector for $M$ if and only if
\[
	E_\lambda \vec v=(M-\lambda I)\vec v = M\vec v-\lambda \vec v=\vec 0,
\]
or, phrased another way, $\vec v$ is a non-zero vector satisfying $\vec v\in \Null(E_\lambda)$.

We've reduced the problem of finding eigenvectors/values of $M$ to finding the null space of $E_\lambda$,
a related matrix.

\Heading{Characteristic Polynomial}

Let $M$ be an $n\times n$ matrix and define $E_\lambda=M-\lambda I$. Every eigenvector for
$M$ must be in the null space of $E_\lambda$ for some $\lambda$. However, because eigenvectors
must be non-zero, the only chance we have of finding an eigenvector is if $\Null(E_\lambda)\neq \Set{\vec 0}$.
In other words, we would like to know when $\Null(E_\lambda)$ is \emph{non-trivial}.

We're well equipped to answer this question. Because $E_\lambda$ is an $n\times n$ matrix, we know $E_\lambda$ has
a non-trivial null space if and only if $E_\lambda$ is not invertible which is true if and only if $\det(E_\lambda)=0$.
Every $\lambda$ defines a different $E_\lambda$ where eigenvectors could be hiding. By viewing $\det(E_\lambda)$
\emph{as a function of $\lambda$}, we can use our mathematical knowledge of single-variable functions to 
figure out when $\det(E_\lambda)=0$.

The quantity $\det(E_\lambda)$, view as a function of $\lambda$, has a special name---it's
called the \emph{characteristic polynomial}\footnote{ This time the term is traditionally given the English name, rather
than being called the \emph{eigenpolynomial}.}.

\SavedDefinitionRender{CharacteristicPolynomial}

\begin{example}
	Find the characteristic polynomial of $A=\mat{1&2\\3&4}$.

	XXX Finish
\end{example}

For an $n\times n$ matrix $A$, $\Char(A)$ has some nice properties.
\begin{itemize}
	\item $\Char(A)$ is a polynomial\footnote{ A priori, it's not obvious that $\det(A-\lambda I)$
	should be a polynomial as opposed to some other type of function.}.
	\item $\Char(A)$ has degree $n$.
	\item The coefficient of the $\lambda^n$ term in $\Char(A)$ is $\pm1$.
	\item $\Char(A)$ evaluated at $\lambda = 0$ is $\det(A)$.
	\item The roots of $\Char(A)$ are precisely the eigenvalues of $A$.
\end{itemize}
We will just accept these properties as facts, but each of them can be proved with the tools we've developed.

\Heading{Using the Characteristic Polynomial to find Eigenvalues}

With the characteristic polynomial in hand, finding eigenvectors/values becomes easier.

\begin{example}
	Find the eigenvectors/values of $A=\mat{1&2\\3&2}$.

	XXX Finish
\end{example}

Using the characteristic polynomial, we can show that every eigenvalue
for a matrix is a root of some polynomial (the characteristic polynomial).
In general, finding roots of polynomials is a hard problem\footnote{ In fact,
numerically approximating eigenvalues turns out to be easier than finding roots
of a polynomial, so many numerical root finding algorithms actually create a matrix
with an appropriate characteristic polynomial and use numerical linear
algebra to approximate its roots.}, and it's not one we will focus on. However, it's
handy to have the quadratic formula in you're back pocket for factoring particularly
stubborn polynomials.

\begin{example}
	Find the eigenvectors/values of $A=\mat{1&2\\3&4}$.

	XXX Finish
\end{example}

\Heading{Transformations without Eigenvectors}

Are there linear transformations without eigenvectors? Well, it
depends on exactly what you mean. Let $\mathcal R:\R^2\to\R^2$
be rotation counter-clockwise by $90^\circ$. Are there any non-zero
vectors that don't change direction when $\mathcal R$ is applied?
Certainly not.

Let's examine further. We know $R=\mat{0&-1\\1&0}$ is a matrix for $\mathcal R$,
and
\[
	\Char(R) = \lambda^2+1.
\]
The polynomial $\lambda^2+1$ has no real roots, which means that $R$ (and $\mathcal R$) have
no real eigenvalues. However, $\lambda^2+1$ does have \emph{complex} roots of $\pm i$.
So far, we've always though of scalars as real numbers, but if we allow complex numbers as
scalars and view $\mathcal R$ as a transformation from $\C^2\to\C^2$, it would have eigenvalues
and eigenvectors.

Complex numbers play an envaluable role in advanced linear algebra and applications of linear
algebra to physics. We will leave the following theorem as foot for thought\footnote{ The theorem
is a direct corrolary of the fundamental theorem of algebra.}.

\begin{theorem}
	If $A$ is a square matrix, then $A$ always has an eigenvalue provided complex eigenvalues are permitted.
\end{theorem}
