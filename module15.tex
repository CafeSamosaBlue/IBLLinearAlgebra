From here on out, we will only be considering linear transformations with the same domain
and codomain (i.e., transformations $\mathcal T:\R^n\to\R^n$). Why? Because that will allow us
to \emph{compare} input and output vectors. From this perspective, a linear transformation $\mathcal T:\R^n\to\R^n$
may stretch, twist, shear, rotate, and project vectors. 

XXX Figure with stretched and sheared vectors

It's the stretched vectors that we're most interested in now. If $\mathcal T$ stretches the vector $\vec v$,
then $\mathcal T$, in that direction, can be described by $\vec v\mapsto \alpha\vec v$, which is an easy-to-understand
linear transformation. The ``stretch'' directions for a linear transformation have a special name---\emph{eigen directions}---and
the vectors that are stretched are called \emph{eigenvectors}.

\SavedDefinitionRender{Eigenvector}

The word \emph{eigen} is German for characteristic, representative, or intrinsic, and
we will see that eigenvectors provide one of the best contexts in which to understand a linear transformation.

\begin{example}
	Let $\mathcal P:\R^2\to\R^2$ be projection onto the line $\ell$ given by $y=x$.
	Find the eigenvectors and eigenvalues of $\mathcal P$.

	We are looking for vectors $\vec v\neq \vec 0$ such that $\mathcal P\vec v=\lambda \vec v$ for some $\lambda$.
	Since $\mathcal P(\ell)=\ell$, we know for any $\vec v\in \ell$
	\[
		\mathcal P(\vec v)=1\vec v=\vec v.
	\]
	Therefore, any non-zero multiple of $\mat{1\\1}$ is an eigenvector for $\mathcal P$ with corresponding
	eigenvalue $1$.

	By considering the null space of $\mathcal P$, we see, for example,
	\[
		\mathcal P\mat{1\\-1}=\mat{0\\0}=0\mat{1\\-1},
	\]
	and so $\mat{1\\-1}$ and all its non-zero multiples are eigenvectors of $\mathcal P$ with corresponding
	eigenvalue $0$.
\end{example}

\Heading{Finding Eigenvectors}

Sometimes you can find the eigenvectors/values of a linear transformation just by thinking about it.
For example, for reflections, projections, and dilations eigenvectors, the eigen directions
are geometrically clear. However, for an arbitrary matrix transformation, it may not be obvious.

Our goal now will be to see if we can leverage our other linear algebra knowledge to find eigenvectors/values,
particularly for matrix transformations. So that we don't have to switch back and forth
between thinking about linear transformations and thinking about matrices, let just think about matrices for now.

Let $M$ be a square matrix. The vector $\vec v\neq \vec 0$
is an eigenvector for $M$ if and only if there exists a scalar $\lambda$ so that
\begin{equation}
	\label{EQEIGEN}
	M\vec v=\lambda \vec v.
\end{equation}
Put another way, $\vec v\neq \vec 0$ is an eigenvector for $M$ if and only if
\[
	M\vec v-\lambda \vec v=(M-\lambda I)\vec v=\vec 0.
\]
The middle equation provides a key insight. The operation $\vec v\mapsto M\vec v-\lambda\vec v$ can be achieved
by multiplying $\vec v$ by the single matrix $E_\lambda=M-\lambda I$.

Now we have that $\vec v\neq \vec 0$ is an eigenvector for $M$ if and only if
\[
	E_\lambda \vec v=(M-\lambda I)\vec v = M\vec v-\lambda \vec v=\vec 0,
\]
or, phrased another way, $\vec v$ is a non-zero vector satisfying $\vec v\in \Null(E_\lambda)$.

We've reduced the problem of finding eigenvectors/values of $M$ to finding the null space of $E_\lambda$,
a related matrix.

\Heading{Characteristic Polynomial}

Let $M$ be an $n\times n$ matrix and define $E_\lambda=M-\lambda I$. Every eigenvector for
$M$ must be in the null space of $E_\lambda$ for some $\lambda$. However, because eigenvectors
must be non-zero, the only chance we have of finding an eigenvector is if $\Null(E_\lambda)\neq \Set{\vec 0}$.
In other words, we would like to know when $\Null(E_\lambda)$ is \emph{non-trivial}.

We're well equipped to answer this question. Because $E_\lambda$ is an $n\times n$ matrix, we know $E_\lambda$ has
a non-trivial null space if and only if $E_\lambda$ is not invertible which is true if and only if $\det(E_\lambda)=0$.
Every $\lambda$ defines a different $E_\lambda$ where eigenvectors could be hiding. By viewing $\det(E_\lambda)$
\emph{as a function of $\lambda$}, we can use our mathematical knowledge of single-variable functions to 
figure out when $\det(E_\lambda)=0$.

The quantity $\det(E_\lambda)$, view as a function of $\lambda$, has a special name---it's
called the \emph{characteristic polynomial}\footnote{ This time the term is traditionally given the English name, rather
than being called the \emph{eigenpolynomial}.}.

\SavedDefinitionRender{CharacteristicPolynomial}

\begin{example}
	Find the characteristic polynomial of $A=\mat{1&2\\3&4}$.
	
	By the definition of Characteristic polynomial of A, we have: 
	\begin{align*}
	    \chr(A) &=\det(A-\lambda I)\\
	            &=\det(\mat{1&2\\3&4}-\mat{\lambda&0\\0&\lambda})=\det(\mat{1-\lambda&2\\3&4-\lambda}\\
	            &=(1-\lambda)(4-\lambda) - 6=\lambda^2 -5\lambda-2,
	\end{align*}
	Thus, we get the characteristic polynomial of A as $\Char(A)=\lambda^2 -5\lambda-2$.
	
	
\end{example}

For an $n\times n$ matrix $A$, $\Char(A)$ has some nice properties.
\begin{itemize}
	\item $\Char(A)$ is a polynomial\footnote{ A priori, it's not obvious that $\det(A-\lambda I)$
	should be a polynomial as opposed to some other type of function.}.
	\item $\Char(A)$ has degree $n$.
	\item The coefficient of the $\lambda^n$ term in $\Char(A)$ is $\pm1$.
	\item $\Char(A)$ evaluated at $\lambda = 0$ is $\det(A)$.
	\item The roots of $\Char(A)$ are precisely the eigenvalues of $A$.
\end{itemize}
We will just accept these properties as facts, but each of them can be proved with the tools we've developed.

\Heading{Using the Characteristic Polynomial to find Eigenvalues}

With the characteristic polynomial in hand, finding eigenvectors/values becomes easier.

\begin{example}
	Find the eigenvectors/values of $A=\mat{1&2\\3&2}$.

	Similarly to the above Example, we first determinate $\Char(A)$:
	\begin{align*}
	    \Char(A)&=\Char(\mat{1-\lambda&2\\3&2-\lambda})\\
	            &=(1-\lambda)(2-\lambda)-6=\lambda^2-3\lambda-4=(\lambda-4)(\lambda+1)
	\end{align*}
	Second, we set the $\chr(A)=0$ and get two roots of $\Char(A)$ as $\lambda_1=-1$, $\lambda_2=4$. Now refer to the definition of eigenvector with $\lambda_1=-1$:
	$$A\vec v=\lambda_1 \vec v$$
	\begin{align*}
	   &\xrightarrow{} \mat{1&2\\3&2}\mat{v_1\\v_2}=-1\mat{v_1\\v_2}\\
	   &\xrightarrow{} \mat{v_1 +2v_2\\3v_1 +2v_2}=\mat{-v_1\\-v_2}\\
	   &\xrightarrow{} \mat{2v_1 +2v_2\\3v_1 +3V_2}= \mat{o\\o}\\
	   &\xrightarrow{} v_1=-v_2;
	\end{align*}
	Thus, one eigenvector of A with eigenvalue $\lambda_1=-1$ is $\mat{1\\-1}$. More general, vectors $\vec v = k \mat{1\\-1}$, $\forall k \in \R$, $k \neq 0$ are eigenvectors of A with eigenvalue $\lambda_1=-1$. 
	
	Similarly, the eigenvectors of $\lambda_2=4$ is $\vec v_2 = m\mat{2\\3}$, $\forall m \in \R$, $m \neq 0$.
	
\end{example}

Using the characteristic polynomial, we can show that every eigenvalue
for a matrix is a root of some polynomial (the characteristic polynomial).
In general, finding roots of polynomials is a hard problem\footnote{ In fact,
numerically approximating eigenvalues turns out to be easier than finding roots
of a polynomial, so many numerical root finding algorithms actually create a matrix
with an appropriate characteristic polynomial and use numerical linear
algebra to approximate its roots.}, and it's not one we will focus on. However, it's
handy to have the quadratic formula in you're back pocket for factoring particularly
stubborn polynomials.

\begin{example}
	Find the eigenvectors/values of $A=\mat{1&2\\3&4}$.

	First, find the $\Char(A)$ and set it equal to 0:
	\begin{align*}
	    \Char(A)&=\Char(\mat{1-\lambda&2\\3&4-\lambda})\\
	            &=(1-\lambda)(4-\lambda)-6=\lambda^2-5\lambda-2=0
	\end{align*}
	By the quadratic formula $r=\frac{-b\pm\sqrt{b^2-4ac}}{2a}$, we have:
	$$\lambda-1=\frac{5-\sqrt{17}}{2},\quad\lambda_2=\frac{5+\sqrt{17}}{2}$$
	Thus for $\lambda_1=\frac{5-\sqrt{17}}{2}$,follow the definition of eigenvectors (but with a little difference):
	$$A\vec v=\lambda\vec v \quad\xrightarrow{}\quad(A-\lambda I)\vec v=\vec 0$$
	Focusing on $A - \lambda I$:
	\begin{align*}
	    \mat{1-\frac{5-\sqrt{17}}{2}&2\\3&4-\frac{5-\sqrt{17}}{2}} & \sim
	    \mat{\frac{-3+\sqrt{17}}{2}&2\\3&\frac{3+\sqrt{17}}{2}}\\
	    & \sim \mat{-3+\sqrt{17}&4\\6&3+\sqrt{17}}
	\end{align*}
	By inspection, we noticed that $\vec v_1 = \mat{4\\3-\sqrt{17}}$ will be mapped to $\vec 0$ by $(A-\lambda I)$\footnote{In practice, One trick in calculate a 2x2 matrix's eigenvector is determinate $(A-\lambda I)$, and then swap the row entry with one of them multiply by -1. Numerically: 
	$$(A-\lambda I) = \mat{a&b\\k*a&k*b}\quad\xrightarrow{}\quad\vec v=\mat{b\\-a}$$
	You may interrupt it as each row vector of $(A-\lambda I)$ dot product $\vec v$ needs to be 0 so that $(A-\lambda I) \vec v=\vec 0$. Thus you may try to inspect for nxn matrix.}. Similarly, we will claim that $\vec v_2=\mat{4\\3+\sqrt{17}}$ is one eigenvector with eigenvalue $\lambda_2 = \frac{5+\sqrt{17}}{2}$.
\end{example}

