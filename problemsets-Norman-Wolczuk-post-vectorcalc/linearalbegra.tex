\documentclass{problemset}
\usepackage{amsmath}

\usepackage{lipsum}
%\usepackage{showframe}
\usepackage{layout}


\usepackage[charter,cal=cmcal]{mathdesign} %different font
\usepackage{microtype}
\usepackage{amsmath}
%\usepackage{amsfonts}
%\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[inline]{enumitem}
\usepackage{xparse}
\usepackage{ifthen}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{tikz}
\usepackage{fancyhdr}
\usepackage{calc}
\usepackage[hidelinks]{hyperref}

\usepackage{pgfplots}
\pgfplotsset{compat=newest}
%%%
% Useful Linear Algebra macros
%%%
\newcommand{\ul}{$\underline{\phantom{xxx}}$}
\newcommand{\ull}{\underline{\phantom{xxx}}}
\newcommand{\xh}{{\hat {\mathbf x}}}
\newcommand{\yh}{{\hat {\mathbf y}}}
\newcommand{\zh}{{\hat {\mathbf z}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\proj}{\mathrm{proj}}
\newcommand{\Proj}{\mathrm{proj}}
\newcommand{\Perp}{\mathrm{perp}}
\renewcommand{\span}{\mathrm{span}\,}
\newcommand{\Span}{\mathrm{span}\,}
\newcommand{\Img}{\mathrm{img}\,}
\newcommand{\Null}{\mathrm{null}\,}
\newcommand{\Range}{\mathrm{range}\,}
\newcommand{\rref}{\mathrm{rref}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\Rank}{\mathrm{rank}}
\newcommand{\nnul}{\mathrm{nullity}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\chr}{\mathrm{char}}
\renewcommand{\d}{\mathrm{d}}

%\tcbuselibrary{skins}
%\usetikzlibrary{shadings}


%%%
% Set up the margins to use a fairly large area of the page
%%%
\textwidth=6in
\topmargin=-1in
\textheight=10in
\parskip=.07in
\parindent=0in

\begin{document}
\pagestyle{empty}

\begin{center}
{\huge\bf Inquiry Based Vector Calculus}\\

\vspace{.7in}
{
\it \copyright\,Jason Siefken, 2016 \\
Creative Commons By-Attribution Share-Alike\, \makebox(30,5){\includegraphics[height=1.2em]{by-sa.pdf}}
}
\end{center}

\section*{About the Document}

	This document was originally designed in the spring of 2016 to guide students
	through an ten week Linear Algebra course (Math 281-3) at
	Northwestern University.  

	A typical class day using the problem-sets:
	\begin{enumerate}
		\item {\bf Introduction by instructor.} This may involve giving a definition,
			a broader context for the day's topics, or answering questions.
		\item {\bf Students work on problems.} Students work individually or in pairs
			on the prescribed problem.  During this time the instructor moves around
			the room addressing questions that students may have and giving one-on-one
			coaching.
		\item {\bf Instructor intervention.} If most students have successfully solved the 
			problem, the instructor regroups the class by providing a concise 
			explanation so that everyone is ready to move to the next concept.  This
			is also time for the instructor to ensure that everyone has understood the
			main point of the exercise (since it is sometimes easy to do some computation
			while being oblivious to the larger context).

			If students are having trouble, the instructor can give hints to the group,
			and additional guidance to ensure the students don't get frustrated
			to the point of giving up.
		\item {\bf Repeat step 2.}
	\end{enumerate}

	Using this format, students are working (and happily so) most of the class.
	Further, they are especially primed to hear the insights of the instructor, 
	having already invested substantially into each problem.

	This problem-set is geared towards concepts instead of computation, though some problems
	focus on simple computation.

	{\bf License}  This document is licensed under the Creative Commons
	By-Attribution Share-Alike License.  That means, you are free to use,
	copy, and modify this document provided that you provide attribution
	to the previous copyright holders and you release your derivative work 
	under the same license.  Full text of the license is at \url{http://creativecommons.org/licenses/by-sa/4.0/}

	If you modify this document, you may add your name to the copyright list.  Also,
	if you think your contributions would be helpful to others, consider making a pull
	requestion, or opening an \emph{issue} at 
	\url{https://github.com/siefkenj/IBLLinearAlegbra}


\newpage

\setcounter{page}{1}
\pagestyle{fancy}
\rfoot{\footnotesize\it \copyright\,Jason Siefken, 2015--2016 \ \makebox(30,5){\includegraphics[height=1.2em]{by-sa.pdf}}}
\renewcommand{\headrulewidth}{0pt}

\section*{Sets of Vectors}
	\vspace{-2em}
	\question
		Write the following sets in set-builder notation
	\begin{parts}
			\item The subset $A\subseteq \R$ of real numbers larger than $\sqrt{2}$.
			\item The subset $B\subseteq \R^2$ of vectors whose first coordinate
			is twice the second.
	\end{parts}

	\begin{definition}[Unions \& Intersections]
		Two common set operations are \emph{unions} and \emph{intersections}.  
		Let $X$ and $Y$ be sets.

		\hfill\begin{minipage}{\dimexpr\textwidth-3cm}
		\begin{itemize}
			\item[(union)] $X\cup Y = \{a:a\in X\text{ or }a\in Y\}$.
			\item[(intersection)] $X\cap Y = \{a: a\in X\text{ and }a\in Y\}$.
		\end{itemize}
		\end{minipage}
	\end{definition}

	\question
	Let $X=\{1,2,3\}$ and $Y=\{2,3,4,5\}$ and $Z=\{4,5,6\}$.  Compute
	\begin{parts}
		\item $X\cup Y$
		\item $X\cap Y$
		\item $X\cup Y\cup Z$
		\item $X\cap Y\cap Z$
	\end{parts}

	\question
	Draw the following subsets of $\R^2$.
	\begin{parts}
		\item $V=\left\{\vec x\in\R^2:\vec x=\begin{bmatrix}0\\t\end{bmatrix}\text{ for some }t\in\R\right\}$.
		\item $H=\left\{\vec x\in\R^2:\vec x=\begin{bmatrix}t\\0\end{bmatrix}\text{ for some }t\in\R\right\}$.
		\item $J=\left\{\vec x\in\R^2:\vec x=t\begin{bmatrix}1\\1\end{bmatrix}\text{ for some }t\in\R\right\}$.
		\item $V\cup H$.
		\item $V\cap H$.
		\item Does $V\cup H=\R^2$?
	\end{parts}


	\vspace{-1em}
\section*{Linear Combinations, Span, and Linear Independence}
	\vspace{-1em}

	\begin{definition}[Linear Combination]
		A \emph{linear combination} of the vectors $\vec v_1,\vec v_2,\ldots,\vec v_n$ is
		a vector
		\[
			\vec w = \alpha_1\vec v_1+\alpha_2\vec v_2+\cdots+\alpha_n\vec v_n
		\]
		where $\alpha_1,\alpha_2,\ldots,\alpha_n$ are scalars.
	\end{definition}

	\question
	Let $\vec v_1=\begin{bmatrix}1\\1\end{bmatrix}$, $\vec v_2=\begin{bmatrix}1\\-1\end{bmatrix}$, and $\vec w=2\vec v_1+\vec v_2$.
	\begin{parts}
		\item Write the coordinates of $\vec w$.
		\item Draw a picture with $\vec w$, $\vec v_1$, and $\vec v_2$.
		\item Is $\mat{3\\3}$ a linear combination of $\vec v_1$ and $\vec v_2$?
		\item Is $\mat{0\\0}$ a linear combination of $\vec v_1$ and $\vec v_2$?
		\item Is $\mat{4\\0}$ a linear combination of $\vec v_1$ and $\vec v_2$?
		\item Can you find a vector in $\R^2$ that isn't a linear combination of
		$\vec v_1$ and $\vec v_2$?
		\item Can you find a vector in $\R^2$ that isn't a linear combination of
		$\vec v_1$?
	\end{parts}
	
	\begin{definition}[Span]
		The \emph{span} of a set of vectors $V$ is the set of
		all linear combinations of vectors in $V$.  That is,
		\[
			\Span V = \{\vec v:\vec v=\alpha_1\vec v_1+\alpha_2\vec v_2 + \cdots 
			+\alpha_n\vec v_n\text{ for some }\vec v_1,\vec v_2,\ldots,\vec v_n\in V
			\text{ and scalars }\alpha_1,\alpha_2,\ldots,\alpha_n\}.
		\]
	\end{definition}

	\question
	Let $\vec v_1=\mat{1\\1}$, $\vec v_2=\mat{1\\-1}$, and $\vec v_3=\mat{2\\2}$.
	\begin{parts}
		\item Draw $\Span\{\vec v_1\}$.
		\item Draw $\Span\{\vec v_2\}$.
		\item Describe $\Span\{\vec v_1,\vec v_2\}$.
		\item Describe $\Span\{\vec v_1,\vec v_3\}$.
		\item Describe $\Span\{\vec v_1,\vec v_2,\vec v_3\}$.
	\end{parts}

	\question
	Give an example of:
	\begin{parts}
		\item two vectors in $\R^3$ that span a plane;
		\item two vectors in $\R^3$ that span a line;
		\item four vectors in $\R^3$ that span a plane;
		\item a set of 50 vectors in $\R^3$ whose span is the line
		through the origin and the point $\mat{1\\2\\-3}$. \\
	\end{parts}

	In some sets, every vector is essential for computing a span.  In others,
	there are ``excess'' vectors.  This leads us to the concept of 
	linear independence.

	\begin{definition}[Linearly Dependent \& Independent]
		We say $\{\vec v_1,\vec v_2,\ldots,\vec v_n\}$ is
		\emph{linearly dependent} if for at least one $i$,
		\[
			\vec v_i\in\span\{\vec v_1,\vec v_2,\ldots,\vec v_{i-1},
			\vec v_{i+1},\ldots,\vec v_n\},
		\]
		and a set is \emph{linearly independent} otherwise.
	\end{definition}

	\question
		Let $\vec u=\mat{1\\0\\0}$, $\vec v=\mat{0\\1\\0}$, and $\vec w=\mat{1\\1\\0}$.
	\begin{parts}
		\item Describe $\Span\{\vec u,\vec v,\vec w\}$.
		\item Is $\{\vec u,\vec v,\vec w\}$ linearly independent?  Why or why not?
	\end{parts}

	Let $X=\{\vec u,\vec v,\vec w\}$.

	\begin{parts}[resume]
		\item Give a subset $Y\subseteq X$ so that $\Span Y=\Span X$ and $Y$ is
		linearly independent.
		\item Give a subset $Z\subseteq X$ so that $\Span Z=\Span X$ and $Z$ is
		linearly independent and $Z\neq Y$.
	\end{parts}
	
	\begin{definition}[Trivial Linear Combination]
	We say a linear combination 
	$a_1\vec v_1+a_2\vec v_2+\cdots +a_n\vec v_n$
	is \emph{trivial} if $a_1=a_2=\cdots=a_n=0$.
	\end{definition}
	
	\question
		Recall $\vec u=\mat{1\\0\\0}$, $\vec v=\mat{0\\1\\0}$, and $\vec w=\mat{1\\1\\0}$.
	\begin{parts}
		\item Consider the linearly dependent 
		set $\{\vec u,\vec v,\vec w\}$ (where $\vec u,\vec v,\vec w$
		are defined as above).  Can you write $\vec 0$
		as a non-trivial linear combination of vectors in this set?
		\item Consider the linearly independent 
		set $\{\vec u,\vec v\}$.  Can you write $\vec 0$
		as a non-trivial linear combination of vectors in this set?
	\end{parts}

	We now have an equivalent definition of linear dependence.

	\begin{definition}[Linearly Dependent \& Independent]
	$\{\vec v_1,\vec v_2,\ldots,\vec v_n\}$ is
	\emph{linearly dependent} if there is a non-trivial
	linear combination of $\vec v_1,\ldots,\vec v_n$ that
	equals the zero vector.
	\end{definition}

	\question
	\begin{parts}
		\item Explain how this new definition implies the old one.
		\item Explain how the old definition implies this new one.
	\end{parts}

	Since we have old def $\implies$ new def, and new def $\implies$ old def ($\implies$
	should be read aloud as `implies'), the two definitions
	are \emph{equivalent} (which we write as new def $\iff$ old def).


	\question
	Suppose for some unknown $\vec u, \vec v, \vec w$, and $\vec a$,
	\[
		\vec a = 3\vec u+2\vec v +\vec w\qquad \text{and}\qquad 
		\vec a = 2\vec u+\vec v -\vec w.
	\]
	\begin{parts}
		\item Could the set $\{\vec u,\vec v,\vec w\}$ be linearly
		independent?
	\end{parts}
	Suppose that
	\[
		\vec a = \vec u+6\vec r-\vec s
	\]
	is the \emph{only} way to write $\vec a$ using $\vec u,\vec r,\vec s$.
	\begin{parts}[resume]
		\item Is $\{\vec u,\vec r,\vec s\}$ linearly independent?
		\item Is $\{\vec u,\vec r\}$ linearly independent?
		\item Is $\{\vec u,\vec v,\vec w,\vec r\}$ linearly independent?
	\end{parts}

\section*{Subspaces and Bases}
	\vspace{-1em}
	\begin{definition}[Subspace]
		A \emph{subspace} $V\subseteq \R^n$ is a subset such that
		\begin{enumerate}
			\item[(i)] $\vec u,\vec v\in V$ implies $\vec u+\vec v\in V$.
			\item[(ii)] $\vec u\in V$ implies $k\vec u\in V$ for all scalars $k$.
		\end{enumerate}
	\end{definition}

	Subspaces give a mathematically precise definition of a ``flat space through the origin.''

	\question
	For each set, draw it and explain whether or not it is a subspace of $\R^2$.
	\begin{parts}
		\item $A=\{\vec x\in\R^2:\vec x=\mat{a\\0}\text{ for some }a\in\Z\}$.
		\item $B=\{\vec x\in\R^2:\vec x\neq \mat{0\\0}\}$.
		\item $C=\{\vec x\in\R^2:\vec x=\mat{0\\t}\text{ for some }t\in\R\}$.
		\item $D=\{\vec x\in\R^2:\vec x=\mat{0\\t}+\mat{1\\1}\text{ for some }t\in\R\}$.
		\item $E=\{\vec x\in\R^2:\vec x=\mat{0\\t}\text{ or }\vec x=\mat{t\\0}\text{ for some }t\in\R\}$.
		\item $F=\{\vec x\in\R^2:\vec x=t\mat{3\\1}\text{ for some }t\in\R\}$.
		\item $G=\span\left\{\mat{1\\1}\right\}$.
		\item $H=\span\{\vec u,\vec v\}$ for some unknown vectors $\vec u,\vec v\in\R^2$.
	\end{parts}

	\begin{definition}[Basis]
		A \emph{basis} for a subspace $V$ is a linearly independent set of vectors, $\mathcal B$,
		so that $\Span\mathcal B=V$.
	\end{definition}

	\question
	Let $\vec u=\mat{1\\0\\0}$, $\vec v=\mat{0\\1\\0}$, $\vec w=\mat{1\\1\\0}$, and $V=\span\{\vec u,\vec v,\vec w\}$.
	\begin{parts}
		\item Describe $V$.
		\item Is $\{\vec u,\vec v,\vec w\}$ a basis for $V$?  Why or why not?
		\item Give a basis for $V$.
		\item Give another basis for $V$.
		\item Is $\Span\{\vec u,\vec v\}$ a basis for $V$?  Why or why not?
	\end{parts}

	\begin{definition}[Dimension]
		The \emph{dimension} of a subspace $V$ is the number of elements in a basis for $V$.
	\end{definition}

	\begin{parts}[resume]
		\item What is the dimension of $V$?
	\end{parts}


	\question
	Let $\vec a=\mat{1\\2\\3}$, $\vec b=\mat{4\\5\\6}$, $\vec c=\mat{7\\8\\8}$ and 
	let $P=\span\{\vec a,\vec b\}$ and $Q=\span\{\vec b,\vec c\}$.
	\begin{parts}
		\item Give a basis for and the dimension of $P$.
		\item Give a basis for and the dimension of $Q$.
		\item Is $P\cap Q$ a subspace? If so, give a basis for it and its dimension.
		\item Is $P\cup Q$ a subspace? If so, give a basis for it and its dimension.
	\end{parts}

	\newpage


\section*{Systems of Linear Equations}
	
	\emph{Linear equations} are equations only involving variables, 
	multiplication by constants, and addition/subtraction.  \emph{Systems}
	of equations are sets of equations that share common variables.

	\question
	Consider the system
	\begin{equation}\label{eq2}
		\begin{array}{rcrl}
			x &-&y &= 2\\
			2x &+&y &= 1
		\end{array}
	\end{equation}

	\begin{parts}
		\item Draw the lines in (\ref{eq2}) on the same coordinate plane.
		\item Algebraically solve the system (\ref{eq2}).  What does this 
		solution represent on your graph?
	\end{parts}
	
	\question
	Let $L$ be the line given by $x-y=2$.
	\begin{parts}
		\item Write an equation of a line that doesn't intersect $L$.
		\item Write an equation of a line that intersects $L$ in 
		\begin{enumerate}
			\item one place.
			\item infinitely many places
			\item exactly two places
		\end{enumerate}
		or explain why no such equation exists.
		\item For each equation you came up with, solve the system algebraically.
		How can you tell algebraically how many solutions there are?
	\end{parts}

\subsection*{The Row Reduction Algorithm}

	\question
	\begin{parts}
		\item Solve the system
		\begin{equation}\label{eq3}
			\begin{array}{rcrcrl}
				x&-&y&-&2z &= -5\\
				2x&+&3y&+&z &= 5\\
				0x&+&2y&+&3z &= 8
			\end{array}
		\end{equation}
		any way you like.

		\item Use an augmented matrix to solve the system (\ref{eq3}).
	\end{parts}

	The system (\ref{eq3}) can be interpreted in two ways (and switching between these 
	interpretations when appropriate is one of the most powerful tools of Linear 
	Algebra).  We can think of solutions to (\ref{eq3})
	as the intersection of three planes, or we can interpret the solution
	as coefficients of a linear combination.

	\begin{parts}[resume]
		\item Rewrite (\ref{eq3}) as a vector equation of the form
		\[
			x\vec v_1+y\vec v_2+z\vec v_3 = \vec p
		\]
		where $x,y,z$ are interpreted as scalar quantities.

		\item If $(x,y,z)$ is a solution to (\ref{eq3}), explain how to get from the
		origin to $\vec p$ using only $\vec v_1, \vec v_2, \vec v_3$.
		\item If $(x,y,z)$ is a solution to \eqref{eq3}, is $\vec p\in\Span\{\vec v_1,\vec v_2,\vec v_3\}$?
	\end{parts}

	\question
	Consider the augmented matrix
	\[
		A=\left[\begin{array}{ccc|c}
			1 & 2 & -1 & -7\\
			0 & 2 & 3 & 9\\
			0 & 0 & 1 & 1
		\end{array}\right].
	\]
	\begin{parts}
		\item Write the system of equations corresponding to $A$.
		\item Solve the system of equations corresponding to $A$.
	\end{parts}

\subsection*{Infinite Solutions}
	\question
	Consider the system
	\begin{equation}\label{eq4}
		\begin{array}{rcrl}
			x&+&2y &= 3\\
			2x&+&4y &= 6
		\end{array}
	\end{equation}

	\begin{parts}
		\item How many solutions does (\ref{eq4}) have?
		\item Write the solutions to (\ref{eq4}) in vector form.
		\item What happens when you use an augmented matrix
		to solve (\ref{eq4})?
	\end{parts}


\subsection*{Free Variables}
	\question
	Suppose the row-reduced augmented matrix corresponding to 
	a system is
	\[
		B=\left[\begin{array}{cc|c}
			1 & 2 & 3\\
			0 & 0 & 0
		\end{array}\right].
	\]
	After reducing, we have 1 equation and 2 unknowns, so we can make
	$2-1=1$ choices when writing a solution.  Let's make the
	choice $y=t$.
	
	\begin{parts}
		\item With the added equation $y=t$, solve the
		system represented by $B$.
	\end{parts}

	\question
	Consider the system given by the augmented matrix
	\[
		C=\left[\begin{array}{ccccc|c}
			1&0&1&2&0&-1\\
			0&1&1&0&0&3\\
			0&0&0&0&1&4
		\end{array}\right].
	\]
	and call the variables in this system $x_1,x_2,
	x_3,x_4,x_5$.

	\begin{parts}
		\item Write the system of equations represented by $C$.
		\item Identify how many choices you can make when writing
		down a solution corresponding to $C$.
		\item Add one equation (of the form $x_i=t$ or $x_j=s$, etc.)
		for each choice you must make when solving the system.
		\item Write in vector form all solutions to $C$.
	\end{parts}

	\question
	\begin{parts}
		\item An unknown system $U$ is represented by an augmented
		matrix with 4 rows and 6 columns.  What is 
		the minimum number of
		free variables solutions to $U$ will have?
		\item An unknown system $V$ is represented by an augmented
		matrix with 6 rows and 4 columns.  What is 
		the minimum number of
		free variables solutions to $V$ will have?
	\end{parts}
	
	\question
	\begin{definition}[Homogeneous]
		A system is called \emph{homogeneous} if all equations equal $0$.
	\end{definition}

		Let $A$ be an unknown system of $3$ equations and $3$ variables and suppose
		 $(x,y,z)=(1,2,1)$ and
		$(x,y,z)=(-1,1,1)$ are solutions to $A$.
	\begin{parts}
		\item Can you produce another solution
		to the system?

		\item  Can you
		produce a solution to the homogeneous version of $A$ (the version of $A$ where every
		equation equals 0)?

		\item Suppose when you use an augmented matrix to solve the system $A$, you only have 
		one free variable.  Could $A$ be homogeneous?  Can you produce all solutions to the system $A$?
	\end{parts}



\section*{Rank}
	\begin{definition}[Rank]
		The \emph{rank} of the matrix $A$ is the number of leading ones in the 
		reduced row echelon form of $A$.
	\end{definition}

	\question
	\begin{parts}
		\item Determine the rank of
		\begin{enumerate*}
			\item $\mat{1&1\\2&2}$
			\item $\mat{1&2\\3&4}$
			\item $\mat{1&1&0\\0&0&1}$
			\item $\mat{3\\3\\2}$
			\item $\mat{1&0&1\\0&1&0\\0&0&1}$.
		\end{enumerate*}
	\end{parts}
	
	\question
	Consider the homogeneous system 
		\begin{equation}\label{eq4b}
			\begin{array}{llll}
				x&+2y&+z &= 0\\
				x&+2y&+3z &= 0\\
				-x&-2y&+z &= 0
			\end{array}
		\end{equation}
	and the non-augmented matrix of coefficients $A=\mat{1&2&1\\1&2&3\\-1&-2&1}$.
	\begin{parts}
		\item What is $\Rank(A)$?
		\item Give the general solution to \eqref{eq4b}.
		\item Are the column vectors of $A$ linearly independent?
		\item Give a non-homogeneous system with the same coefficients as \eqref{eq4b} that has
			\begin{enumerate}
				\item infinitely many solutions
				\item no solutions.
			\end{enumerate}
	\end{parts}

	\question
	\begin{parts}
		\item The rank of a $3\times 4$ matrix $A$ is $3$.  Are the column vectors of $A$ linearly independent?
		\item The rank of a $4\times 3$ matrix $B$ is $3$.  Are the column vectors of $B$ linearly independent?
	\end{parts}

\section*{Span Again}
	\question
	Consider the system 
		\begin{equation}\label{eq4bc}
			\begin{array}{llll}
				x&-y&-z &= 0\\
				0x&+1y&+2z &= 0\\
				3x&-3y&+3z &= 0
			\end{array}
		\end{equation}
	which has the unique solution $(x,y,z)=(0,0,0)$.
	\begin{parts}
		\item Give vectors $\vec u,\vec v,\vec w$ so that the system \eqref{eq4bc}
			corresponds to the vector equation $x\vec u+y\vec v+z\vec w = \vec 0$.
		\item Is $\vec w\in\Span\{\vec u,\vec v\}$? If so, write it as a linear combination
			of $\vec u$ and $\vec v$.
	\end{parts}

	The matrix $M$ is the non-augmented matrix corresponding to a homogeneous system of linear equations.
	$M$ also corresponds to the vector equation $x\vec a+y\vec b+z\vec c=\vec 0$.  Further, we know
	\[
		\rref(M) = \mat{1&0&1\\0&1&-2\\0&0&0}.
	\]
	\begin{parts}[resume]
		\item Give a solution to the vector equation $x\vec a+y\vec b+z\vec c=\vec 0$.
		\item Is $\vec c\in\Span\{\vec a,\vec b\}$?  If so, write it as a linear combination
			of $\vec a$ and $\vec b$.
		\item Do you have enough information to tell if $\{\vec a,\vec b\}$ is linearly independent?  Why or why not?
	\end{parts}

\subsection*{Finding Linearly Independent Subsets}
	\question
	Suppose when you use an augmented matrix to solve
	$a\vec u+b\vec v+c\vec w=\vec 0$ you have no free variables.
	
	\begin{parts}
		\item Is $\{\vec u,\vec v,\vec w\}$ linearly independent?
	\end{parts}
	
	Suppose when you use an augmented matrix to solve
	$a\vec u+b\vec v+c\vec w=\vec 0$, the second column corresponds to a 
	free variable.
	
	\begin{parts}[resume]
		\item Is $\{\vec u,\vec v,\vec w\}$ linearly independent?
		\item Is $\{\vec u,\vec w\}$ linearly independent?
		\item Is $\{\vec u,\vec v\}$ linearly independent?
	\end{parts}

	\begin{definition}[Maximal Linearly Independent Subset]
	Given a set of vectors $X$, a 
	\emph{maximal linearly independent subset} of $X$ is a linearly independent
	subset $V\subseteq X$ with the most possible vectors in it 
	(i.e., if you took any subset of $X$ with more vectors, it would be linearly
	dependent).
	\end{definition}

	\question
	\begin{parts}
		\item Give a maximal linearly independent subset, $T$, of
		$\left\{\mat{a\\b\\c}:a,b,c\in \R\right\}$.
		\item What is the size of $T$?
	\end{parts}

	\question
	Consider the vectors
	\[
		\vec v_1=\mat{1\\2\\1}
		\qquad
		\vec v_2=\mat{-1\\-1\\-1}
		\qquad
		\vec v_3=\mat{0\\1\\0}
		\qquad
		\vec v_4=\mat{-1\\2\\0}
		\qquad
		\vec v_5=\mat{1\\-1\\1}
	\]
	and the matrices
	\[
		A=\mat{1&-1&0&-1&1\\ 2&-1&1&2&-1\\1 & -1&0&0&1}
		\qquad \rref (A)
		=\mat{1&0&1&0&-2\\0&1&1&0&-3\\0&0&0&1&0}.
	\]
	(Notice that the columns of $A$ are the vectors $\vec v_1,\ldots, \vec v_5$)

	\begin{parts}
		\item Is $V=\{\vec v_1,\vec v_2,\vec v_3,\vec v_4,\vec v_5\}$ linearly
		independent?
		\item Pick a maximal linearly independent subset of $V$.
		\item Pick another (different) maximal linearly independent subset of $V$.
		\item Give a basis for $\Span(V)$.
		\item What is the dimension of $\Span(V)$?
	\end{parts}

	\newpage
\section*{Matrices}
	\question
	\[
		A=\mat{1&2\\3&1\\0&-1}
		\qquad
		B=\mat{-1&-1\\0&1\\1&-2}
		\qquad
		C=\mat{1&2&0\\-1&-1&-1}
	\]
	\begin{parts}
		\item Write the shape of the matrices $A,B,C$ (i.e., for each one,
		write the dimensions in $m\times n$ form).
		\item List \emph{all} products between the matrices $A,B,C$ that are
		defined. (Your list will be some subset of $AB,AC,BA,CA,BC,CB$.)
		\item Compute $AC$ and $CA$.
	\end{parts}

	\question
	\begin{parts}
		\item If the matrices $X$ and $Y$ are both square $n\times n$ matrices,
		does $XY=YX$?  Explain.
		\item If the matrices $X$ and $Y$ are both square $n\times n$ matrices,
		does $X+Y=Y+X$?  Explain.
	\end{parts}

	\question
	Consider the system represented by
	\[
		\mat{1&-3&0\\0&0&1\\0&0&0}\mat{x\\y\\z}=\vec b.
	\]
	\begin{parts}
		\item If $\vec b=\mat{1\\2\\3}$, is the set of solutions to this system a 
		point, line, plane, or other?
		\item If $\vec b=\mat{1\\1\\0}$, is the set of solutions to this system a 
		point, line, plane, or other?
	\end{parts}

	\question
	The entries of a matrix are specified by (row,column) pairs of integers.  If
	$a_{ij}$ is the $(i,j)$ entry of a matrix $A$, we may write $A=[a_{ij}]$.
	\begin{parts}
		\item Write the $2\times 2$ matrix $A$ with entries $a_{11} = 4$, $a_{12}=3$,
			$a_{21} = 7$ and $a_{22}=9$.
		\item Let $B=[b_{ij}]$ be the $3\times 3$ matrix where $b_{ij} = i+j$.  Write $B$.
		\item Let $C=[c_{ij}]$ be the $3\times 4$ matrix where $c_{ij} = 0$ if $i=j$ and
			$c_{ij}=1$ if $i\neq j$.
	\end{parts}

	\question
	\begin{definition}
		The \emph{transpose} of a matrix $A=[a_{ij}]$ is the matrix $A^{T}=[a_{ji}]$.
	\end{definition}
	Visually, the transpose of a matrix swaps rows and columns.

	\[
		A=\mat{1&1&2\\2&2&1}
	\]
	\begin{parts}
		\item What is the shape of $A$ and $A^T$?
		\item Write down $A^T$.
	\end{parts}

	$B$ and $D$ are $4\times 6$ matrices and $C$ is a $6\times 4$ matrix.

	\begin{parts}[resume]
		\item Does $(BC)^T=B^TC^T$? Explain.
		\item Does $(B+D)^T=B^T+D^T$? Explain.
		\item Compute $AA^T$ and $A^TA$ (where $A$ is the matrix defined earlier).
		What do you notice?
	\end{parts}

	\question
	\begin{definition}
		A matrix $X$ is called \emph{symmetric} if $X=X^T$.  
	\end{definition}
	Symmetric matrices have many useful properties,
	and have deep connections with orthogonality and eigenvectors (which we will get to later on).

	\begin{parts}
		\item Prove that if $W$ is a square matrix, then $V=W^TW+W+W^T$ is a symmetric
		matrix.
	\end{parts}

	\question
	\begin{definition}
		A \emph{zero matrix} is a matrix whose entries are all zeros.
		An \emph{identity matrix} is a square matrix whose diagonal
		entries are $1$ and non-diagonal entries are $0$.
	\end{definition}
	We write the $m\times n$ zero matrix as $0_{m\times n}$ or just $0$ if the shape
	is determined by context.  The $n\times n$ identity matrix is notated $I_{n\times n}$ or just
	$I$ if the shape is determined by context.

	Let $A=\mat{1&2&3\\4&5&6\\7&8&9}$.
	\begin{parts}
		\item Write down the $3\times 3$ identity matrix and the $3\times 3$ zero
		matrix.
		\item Compute $I_{3\times 3}A$, $AI_{3\times 3}$, $0_{3\times 3}A$,
		and $A0_{3\times 3}$.
		\item If we were to think of matrices as numbers, what numbers would the
		zero matrix and the identity matrix correspond to?
	\end{parts}

	\question
	\begin{parts}
		\item Solve the matrix equation
		\[
			I_{4\times 4}\mat{x\\y\\z\\w} = \mat{2\\3\\1\\-1}.
		\]
	\end{parts}



\newpage
\section*{Linear Transformations}
\vspace{-1.5em}
	
	\question
	$\mathcal R:\R^2\to\R^2$ is the transformation that rotates vectors counter-clockwise 
	by $90^\circ$.
	\begin{parts}
		\item Compute $\mathcal R\mat{1\\0}$ and $\mathcal R\mat{0\\1}$.
		\item Compute $\mathcal R\mat{1\\1}$.  How does this relate to
			$\mathcal R\mat{1\\0}$ and $\mathcal R\mat{0\\1}$?
		\item What is $\mathcal R\left(a\mat{1\\0}+b\mat{0\\1}\right)$?
		\item Write down a matrix $R$ so that $R\vec v$ is $\vec v$ rotated
			counter clockwise by $90^\circ$.
	\end{parts}

	\question
	$\mathcal S:\R^3\to\R^3$ stretches in the $\zh$ direction  by a factor of $2$
	and contracts in the $\yh$ direction by a factor of $3$.
	\begin{parts}
		\item Write a matrix representation of $\mathcal S$.
	\end{parts}

	\begin{definition}[Linear Transformation]
		If $V$ and $W$ are vector spaces, a function $T:V\to W$ is called a \emph{linear transformation}
		if 
		\[
			T(\vec u+\vec v)=T\vec u+T\vec v \qquad\text{and}\qquad
			T(\alpha \vec v)=\alpha T\vec v
		\]
		for vectors $\vec u,\vec v\in V$ and all scalars $\alpha$.
	\end{definition}

	\question
	\begin{parts}
		\item Classify the following as linear transformation or not
			\begin{enumerate}
				\item $\mathcal R$ from above.
				\item $\mathcal S$ from above.
				\item $W:\R^2\to\R^2$ where $W\mat{x\\y}=\mat{x^2\\y}$.
				\item $T:\R^2\to\R^2$ where $T\mat{x\\y}=\mat{x+2\\y}$.
				\item $\mathcal P:\R^2\to\R^2$ where $\mathcal P\mat{x\\y}=\proj_{\vec u}\mat{x\\y}$ and 
					$\vec u=\mat{2\\3}$.
			\end{enumerate}
	\end{parts}

	It turns out every linear transformation can be written as a matrix (in fact
	this is why matrix multiplication was invented).

	\question
	Define $\mathcal P$ to be projection onto $\vec u=\mat{2\\3}$.
	\begin{parts}
		\item Write down a matrix for $\mathcal P$.
		%\item What is the null space of $\mathcal P$?
		\item What is the rank of the matrix corresponding to $\mathcal P$?
		%\item Is $\mathcal P$ invertible?
	\end{parts}

	Matrix multiplication was designed to exactly model composition of linear transformations.
	\begin{parts}[resume]
		\item Write down a matrix for $\mathcal P$ and for $\mathcal R$, the counter-clockwise rotation
			by $90^\circ$.
		\item Write down matrices for $\mathcal P\circ\mathcal R$ and $\mathcal R\circ \mathcal P$.
	\end{parts}

	\begin{definition}[Range]
		The \emph{range} (or \emph{image}) of a linear transformation $T:V\to W$ is the set of vectors that
		$T$ can output.  That is,
		\[
			\Range(T)=\{\vec y\in W:\vec y=T\vec x\text{ for some }\vec x\in V\}.
		\]
	\end{definition}
	\begin{definition}[Null Space]
		The \emph{null space} (or \emph{kernel}) of a linear transformation $T:V\to W$ is the
		set of vectors that get mapped to zero under $T$.  That is,
		\[
			\Null(T)=\{\vec x\in V:T\vec x=\vec 0\}.
		\]
	\end{definition}

	\question
	Let $\mathcal P:\R^2\to\R^2$ be projection onto the vector $\vec u=\mat{2\\3}$ (like before).
	\begin{parts}
		\item What is the range of $\mathcal P$?
		\item What is the null space of $\mathcal P$?
	\end{parts}

	\question
	Let $T:\R^n\to\R^m$ be an arbitrary linear transformation.
	\begin{parts}
		\item Show that the null space of $T$ is a subspace.
		\item Show that the range of $T$ is a subspace.
	\end{parts}


	\begin{definition}[Fundamental Subspaces]
		Associated with any matrix $M$ are three fundamental subspaces: the \emph{row space}
		of $M$ is the span of the rows of $M$; the \emph{column space} of $M$ is the span
		of the columns of $M$; and the \emph{null space} of $M$ is the set of solutions
		to $M\vec x=\vec 0$. 
		
	\end{definition}

	\question
	Consider $A=\mat{1&0&0\\0&1&0}$.
	\begin{parts}
		\item Describe the row space of $A$.
		\item Describe the column space of $A$.
		\item Is the row space of $A$ the same as the column space of $A$?
		\item Describe the set of all vectors perpendicular to the rows of $A$.
		\item Describe the null space of $A$.
	\end{parts}

	\question
	\[
		B=\mat{1&2&3\\1&1&1}\qquad C=\rref(B)=\mat{1&0&-1\\0&1&2}
	\]
	\begin{parts}
		\item How does the row space of $B$ relate to the row space of $C$?
		\item How does the null space of $B$ relate to the null space of $C$?
		\item Compute the null space of $B$.
	\end{parts}

	\question
	\[
		P=\mat{0&0\\1&2}\qquad Q=\rref(P)=\mat{1&2\\0&0}
	\]
	\begin{parts}
		\item How does the column space of $P$ relate to the column space of $Q$?
		\item Describe the columns space of $P$ and the column space of $Q$.
	\end{parts}

	\begin{theorem}[Rank-nullity Theorem]
	The \emph{nullity} of a matrix is the dimension of the null space.

	The rank-nullity theorem states
	\[
		\rank(A)+\nnul(A) = \#\text{ of columns in }A.
	\]
	\end{theorem}

	\question
	The vectors $\vec u,\vec v\in\R^9$ are linearly independent and $\vec w=2\vec u-\vec v$.
	Define $A=[\vec u|\vec v|\vec w]$.
	\begin{parts}
		\item What is the rank and nullity of $A^T$?
		\item What is the rank and nullity of $A$?
	\end{parts}

\newpage

\section*{Matrix Inverses}

	\question
	\begin{parts}
		\item Apply the row operation $R_3\to R_3+2R_1$ to the $3\times 3$ identity
		matrix and call the result $E_1$.
		\item Apply the row operation $R_3\to R_3-2R_1$ to the $3\times 3$ identity
		matrix and call the result $E_2$.
	\end{parts}

	\begin{definition}
	An \emph{elementary matrix} is the identity matrix with a single row operation applied.
	\end{definition}

	\[
		A=\mat{1&2&3\\4&5&6\\7&8&9}
	\]
	\begin{parts}[resume]
		\item Compute $E_1A$ and $E_2A$.  How do the resulting matrices relate to row
		operations?
		\item Without computing, what should the result of applying the row
		operation $R_3\to R_3-2R_1$ to $E_1$ be?  Compute and verify.
		\item Without computing, what should $E_1E_2$ be?  What about $E_2E_1$?
		Now compute and verify.
	\end{parts}

	\begin{definition}
		The \emph{inverse} of an $n\times n$ matrix $A$ is an $n\times n$
		matrix $B$ such that $AB=I_{n\times n}=BA$.
		In this case, $B$ is called the inverse of $A$ and is notated as $A^{-1}$.
	\end{definition}

	\question
	Consider the matrices 
	\[
		A=\mat{1&2&0\\0&1&0\\-3&-6&1}\qquad
		B=\mat{1&0&0\\0&1&0}\qquad
		C=\mat{1&0\\0&1\\0&0}
	\]
	\[
		D=\mat{1&-2&0\\0&1&0\\3&0&1}\qquad
		E=\mat{1&0&0\\0&2&0\\0&1&1}\qquad
		F=\mat{1&0&0\\0&1&0\\0&0&1}
	\]
	\begin{parts}
		\item Which pairs of matrices above are inverses of each other?
	\end{parts}

	\question
	\[
		B=\mat{1 &4\\0 &2}
	\]
	\begin{parts}
		\item Use two row operations to reduce $B$ to $I_{2\times 2}$
		and write an elementary matrix $E_1$ corresponding to the first operation
		and $E_2$ corresponding to the second.
		\item What is $E_2E_1B$?
		\item Find $B^{-1}$.
		\item Can you outline a procedure for finding the inverse of a matrix
		using elementary matrices?
	\end{parts}

	\question
	\[
		A=\mat{1&2&-1\\2&2&4\\1&3&-3}\qquad
		\vec b=\mat{1\\2\\3}\qquad
		C=[A|\vec b]\qquad
		A^{-1}=\mat{9&-3/2&-5\\-5&1&3\\-2&1/2&1}
	\]
	\begin{parts}
		\item What is $A^{-1}A$?
		\item What is $\rref(A)$?
		\item What is $\rref(C)$? (Hint, there is no need to actually do row reduction!)
		\item Solve the system $A\vec x=\vec b$.
	\end{parts}

	\question
	\begin{parts}
		\item For two square matrices $X,Y$, should $(XY)^{-1}=X^{-1}Y^{-1}$?
		\item If $M$ is a matrix corresponding to a non-invertible linear transformation $T$,
			could $M$ be invertible?
	\end{parts}


\section*{Change of Basis}
	\question
	Let $\vec b_1=\mat{1\\1}$, $\vec b_2=\mat{1\\-1}$, $\vec c=\mat{4\\0}$, and $\mathcal B=\{\vec b_1,\vec b_2\}$.
	\begin{parts}
		\item Is $\mathcal B$ a basis for $\R^2$?
		\item Find coefficients $\alpha_1$ and $\alpha_2$ so that $\vec c=\alpha_1\vec b_1+\alpha_2\vec b_2$.
	\end{parts}
	We call the vector $\mat{\alpha_1\\\alpha_2}$ the representation of $\vec c$ in the $\mathcal B$ basis and notate
	this by $[\vec c]_{\mathcal B}=\mat{\alpha_1\\\alpha_2}$.
	\begin{parts}[resume]
		\item Compute $[\vec e_1]_{\mathcal B}$ and $[\vec e_2]_{\mathcal B}$.
	\end{parts}
	Let $X=[\vec b_1|\vec b_2]$ be the matrix whose columns are $\vec b_1$ and $\vec b_2$.
	\begin{parts}[resume]
		\item Compute $X[\vec c]_{\mathcal B}$.  What do you notice?
	\end{parts}

	
	\question
	Let $\mathcal S=\{\vec e_1,\vec e_2,\ldots,\vec e_n\}$ be the standard basis for $\R^n$.
	Given a basis $\mathcal B=\{\vec b_1,\vec b_2,\ldots,\vec b_n\}$ for $\R^n$, the 
	matrix $X=[\vec b_1|\vec b_2|\cdots|\vec b_n]$ converts
	vectors from the $\mathcal B$ basis into the standard basis.  In other words,
	\[
		X[\vec v]_{\mathcal B} = [\vec v]_{\mathcal S}.
	\]
	\begin{parts}
		\item Should $X^{-1}$ exist? Explain.
		\item Consider the equation\[
				X^{-1}[\vec v]_{?} = [\vec v]_{?}.
			\]
			Can you fill in the ``?'' symbols so that the equation makes sense?
		\item What is $[\vec b_1]_{\mathcal B}$?  How about $[\vec b_2]_{\mathcal B}$?  Can
			you generalize to $[\vec b_i]_{\mathcal B}$?
	\end{parts}

	\question
	Let $\vec c_1=\mat{2\\1}$, $\vec c_2=\mat{5\\3}$, $\mathcal C=\{\vec c_1,\vec c_2\}$, and $A=\mat{2&5\\1&3}$.
	Note that $A^{-1}=\mat{3&-5\\-1&2}$ and that $A$ changes vectors from the $\mathcal C$ basis to the standard
	basis and $A^{-1}$ changes vectors from the standard basis to the $\mathcal C$ basis.
	\begin{parts}
		\item Compute $[\vec c_1]_{\mathcal C}$ and $[\vec c_2]_{\mathcal C}$.
	\end{parts}
	Let $T:\R^2\to\R^2$ be the linear transformation that stretches in the $\vec c_1$ direction by a factor of $2$
	and doesn't stretch in the $\vec c_2$ direction at all.
	\begin{parts}[resume]
		\item Compute $T\mat{2\\1}$ and $T\mat{5\\3}$.
		\item Compute $[T\vec c_1]_{\mathcal C}$ and $[T\vec c_2]_{\mathcal C}$.
		\item Compute the result of $T\mat{\alpha\\\beta}_{\mathcal C}$ and express the result in the
			$\mathcal C$ basis (i.e., as a vector of the form $\mat{?\\?}_{\mathcal C}$).
		\item Find a matrix for $T$ in the $\mathcal C$ basis.
		\item Find a matrix for $T$ in the standard basis.
	\end{parts}
	\begin{definition}[Similar Matrices]
		A matrix $A$ and a matrix $B$ are \emph{similar matrices}, denoted $A\sim B$, if
		$A$ and $B$ represent the same linear transformation but in possibly different bases.
		Equivalently, $A\sim B$ if there is an invertible matrix $X$ so that
		\[
			A=XBX^{-1}.
		\]
	\end{definition}



\newpage
\section*{Determinants}
	\begin{definition}[Unit $n$-cube]
		The unit $n$-cube is the $n$-dimensional cube with side length 1 and lower-left
		corner located at the origin.  That is 
		\[
			C_n = \left\{\vec x\in\R^n:\vec x=\sum_{i=1}^n \alpha_i\vec e_i\text{ for some }\alpha_1,\ldots,\alpha_n\in[0,1]\right\}=[0,1]^n.
		\]
	\end{definition}
	The volume of the unit $n$-cube is always 1.

	\question
	The picture shows what the linear transformation $T$ does to the unit square (i.e., the unit $2$-cube).

	\begin{center}
	\includegraphics[width=2.5in]{images/transform1b.pdf}
	\includegraphics[width=2.5in]{images/transform2b.pdf}
	\end{center}

	\vspace{-6em}
	\begin{parts}
		\item What is $T\mat{1\\0}$, $T\mat{0\\1}$, $T\mat{1\\1}$?
		\item Write down a matrix for $T$.
		\item What is the volume of the image of the unit square (i.e., the volume of $T(C_2)$)?  You may need
			to use trigonometry.
	\end{parts}
	
	\begin{definition}[Determinant]
	The \emph{determinant} of a linear transformation $X:\R^n\to \R^n$ is the 
	oriented volume of the image of the unit $n$-cube.  The determinant
	of a square matrix is the oriented volume of the parallelepiped 
	($n$-dimensional parallelogram) given by the column vectors or the row
	vectors.
	\end{definition}

	\question
	We know the following about the transformation $A$:
	\[
		A\mat{1\\0}=\mat{2\\0}\qquad\text{and}\qquad A\mat{0\\1}=\mat{1\\1}.
	\]
	\begin{parts}
	\item Draw $C_2$ and $A(C_2)$, the image of the unit square
			under $A$.
		\item Compute the area of $A (C_2)$.
		\item Compute $\det(A)$.
	\end{parts}

	\question
	Suppose $R$ is a rotation counterclockwise by $30^\circ$.
	\begin{parts}
	\item Draw $C_2$ and $R(C_2)$.
	\item Compute the area of $R(C_2)$.
		\item Compute $\det(R)$.
	\end{parts}
	
	\question
	We know the following about the transformation $F$:
	\[
		F\mat{1\\0}=\mat{0\\1}\qquad\text{and}\qquad F\mat{0\\1}=\mat{1\\0}.
	\]
	\begin{parts}
		\item What is $\det(F)$?
	\end{parts}

	\question
	\begin{itemize}
		\item $E_f$ is $I_{3\times 3}$ with the first two rows swapped.
		\item $E_m$ is $I_{3\times 3}$ with the third row multiplied by 6.
		\item $E_a$ is $I_{3\times 3}$ with $R_1\to R_1+2R_2$ applied.
	\end{itemize}

	\begin{parts}
		\item What is $\det(E_f)$?
		\item What is $\det(E_m)$?
		\item What is $\det(E_a)$?
		\item What is $\det(E_fE_m)$?
		\item What is $\det(4I_{3\times 3})$?
		\item What is $\det(W)$ where $W=E_fE_aE_fE_mE_m$?
	\end{parts}

	\question
	$U=\mat{1&2&1&2\\0&3&-2&4\\0&0&-1&0\\0&0&0&4}$
	\begin{parts}
		\item What is $\det(U)$?
		\item $V$ is a square matrix and rref$(V)$ has a row of zeros.
		 What is $\det(V)$?
		\item $P$ is projection onto the vector $\mat{-1\\-1}$. What is $\det(P)$?
	\end{parts}

	\question
	Suppose you know $\det(X)=4$.
	\begin{parts}
		\item What is $\det(X^{-1})$?
		\item Derive a relationship between $\det(Y)$
			and $\det(Y^{-1})$ for an arbitrary matrix $Y$.
		\item Suppose $Y$ is not invertible.  What is $\det(Y)$?
	\end{parts}

	After all this work with determinants, we see 
	that (like dot products) there is a geometric and an
	algebraic way of thinking about them, and they 
	\emph{determine} if a matrix is invertible.

	\question
	\begin{parts}
		\item The linear transformation $L:\R^3\to\R^3$ is a change of coordinates and $\det(L)=-4$.
			What is the volume form for this change of coordinates?
		\item Suppose $P:\R^2\to\R^2$ is the parameterization defined by $P\left(\mat{x\\y}\right)=\mat{1&2\\3&9}\mat{x\\y}+\mat{1\\1}$.
			Find the volume form for $P$.
		\item Suppose $p:\R^2\to\R^2$ is the parameterization defined by $p(r,\theta)=(r\cos\theta,r\sin\theta)$.  Find
			a linear approximation to $p$ at the point $(r_0,\theta_0)$.  Use determinants to compute the volume form for $p$
			at $(r_0,\theta_0)$.
	\end{parts}
	\begin{definition}[Jacobian]
		Let $p:\R^n\to\R^n$ be a parameterization.  Let $L_{\vec x_0}(\vec x) = J_{\vec x_0}\vec x+\vec q_{\vec x_0}$
		be the linear approximation to $p$ at the point $\vec x_0$.  The \emph{Jacobian} of $p$ at the point $\vec x_0$
		is defined to be
		\[
			\mathrm{Jacob}_{\vec x_0}(p) = \det(J_{\vec x_0}).
		\]
	\end{definition}

\end{document}
